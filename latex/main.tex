\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}    % Umlaute in der .tex-Datei
\usepackage[T1]{fontenc}       % Korrekte Silbentrennung
\usepackage{lmodern}           % Bessere Schrift
\usepackage{graphicx}          % Für Logo/Grafiken
\usepackage[left=3.5cm, right=2.0cm, top=2.5cm, bottom=2.5cm]{geometry} % Seitenränder anpassen bei Bedarf
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{setspace}
\onehalfspacing
\definecolor{htwgreen}{HTML}{76B900}
\usepackage[ngerman, num]{isodate}
\monthyearsepgerman{\,}{\,}
\usepackage[ngerman]{babel}
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{./references.bib} 

% Für Überschriften etwas schönere Darstellung (optional):
% \usepackage{titlesec}
% \titleformat{\section}{\bfseries\Large}{\thesection}{1em}{}
% \titleformat{\subsection}{\bfseries\normalsize}{\thesubsection}{1em}{}

% \onehalfspacing % 1,5-zeiliger Abstand (ggf. anpassen)

\begin{document}

% Titelseite
\begin{titlepage}
    \centering
    
    \includegraphics[width=4cm]{./bilder/S04_HTW_Berlin_Logo_pos_FARBIG_RGB.jpg}\\[1.0cm]
    \rule{\linewidth}{0.5pt}\\[0.7cm]
    
    {\color{htwgreen}\bfseries\Large Eignung von Large Language Models (LLMs) zur Generierung von Python Code zur
    Datenanalyse}\\[0.5cm]
    \rule{\linewidth}{0.5pt}\\[2.0cm]
    {\large Bachelorarbeit/Masterarbeit}\\[1.5cm]
    
    % Name des Studiengangs, Fachbereich, etc.
    {\large Name des Studiengangs}\\
    {\LARGE Wirtschaftsinformatik}\\[0.3cm]
    {\color{htwgreen}\LARGE \textbf{Fachbereich 4}}\\[1.5cm]
    
    {vorgelegt von}\\
    {\LARGE Maurice Krüger}\\[3cm]
    
    % Datum, Ort
    {\Large Datum:}\\
    Berlin, \today\\[2.5cm]

    % Gutachter
    {\LARGE
    \begin{tabular}{l l}
        Erstgutachter:  & Prof. Dr.-Ing. Ingo Claßen \\
        Zweitgutachter: & Prof. Dr. Axel Hochstein \\
    \end{tabular}
    }

\end{titlepage}

\textbf{
    Abstract
    Diese Bachelorarbeit untersucht die Eignung moderner Large Language Models (LLMs) für die automatisierte Generierung von Python-Code im Kontext typischer Datenanalyseaufgaben. Dabei wird zunächst ein Überblick über die theoretischen Grundlagen von LLMs und der Programmiersprache Python gegeben. Anschließend werden in einer empirischen Untersuchung Codebeispiele durch LLMs erzeugt und mit manuell geschriebenen Skripten verglichen. Die Bewertung erfolgt anhand mehrerer Kriterien wie Korrektheit, Performanz sowie Verständlichkeit des generierten Codes. Die Ergebnisse zeigen, dass LLMs bereits in der Lage sind, einfachen bis mittleren Anforderungen in der Datenanalyse gerecht zu werden. Allerdings treten insbesondere bei komplexeren Analysen und Datenvorverarbeitungsschritten Fehlerrisiken sowie Wartbarkeitsprobleme auf. Abschließend werden Empfehlungen für den praktischen Einsatz von LLMs in der Datenanalyse abgeleitet sowie ein Ausblick auf zukünftige Entwicklungen gegeben.
}
\newpage

% Inhaltsverzeichnis

\tableofcontents
\newpage

% Einleitung

\section{Einleitung}
\subsection{Problemstellung und Forschungsfragen}
\label{sec:forschungsfragen}
Die schnelle Entwicklung von Large Language Models (LLMs), wie zum Beispiel ChatGPT, hat in den letzten Jahren sowohl im privaten als auch im beruflichen Bereich viel Aufmerksamkeit erregt. Ursprünglich wurden LLMs hauptsächlich zur Lösung alltäglicher Probleme und der Verarbeitung und Erzeugung menschlicher Sprache eingesetzt, doch zunehmend zeigt sich, dass sie auch Programmiercode in verschiedenen Sprachen erstellen können. Besonders in der Programmiersprache Python – einer weit verbreiteten Sprache für Datenanalyse und Machine Learning – sind die Fortschritte in der automatisierten Code-Generierung durch LLMs bereits bemerkenswert\cite{NEURIPS2023_43e9d647,chen2021evaluatinglargelanguagemodels}.

Aktuelle Forschungsarbeiten konzentrieren sich auf die systematische Bewertung von solch generierten Codes, um Fehlerquellen und Qualitätsmerkmale zu bemessen. Die Bereitstellung öffentlicher Evaluierungsdatensätze und -frameworks, wie etwa \emph{HumanEval}\cite{chen2021evaluatinglargelanguagemodels} oder \emph{EvalPlus}\cite{evalplus}, ermöglicht standardisierte Vergleichsstudien verschiedener LLMs. Dies eröffnet neue Anwendungsfelder im Bereich der Datenanalyse: Anstatt den Code manuell zu schreiben, könnten Nutzer in Zukunft lediglich ihre Anforderungen in natürlicher Sprache formulieren und das Modell würde diese für den Nutzer umsetzen\cite{nijkamp2023codegenopenlargelanguage}.

Vor diesem Hintergrund stellt sich die Frage, ob und inwiefern LLMs tatsächlich qualitativ hochwertigen Python-Code für datenanalytische Aufgaben erzeugen können und wie dieser Code im Vergleich zu manuell geschriebenen Code abschneidet. Auch die möglichen Grenzen dieser automatisierten Generierung, wie etwa in Bezug auf Performanz, Wartbarkeit oder Fehlerraten, sind hierbei von großer Bedeutung\cite{wang2021codet5identifierawareunifiedpretrained}.

Daraus ergibt sich die zentrale \textbf{Hauptforschungsfrage}:

\begin{quote}
    \emph{Inwieweit eignen sich Large Language Models (LLMs) zur Durchführung gängiger Datenanalyseaufgaben in Python, und wie schneidet dieser Code im Vergleich zu manuell geschriebenen Code hinsichtlich Effizienz, Korrektheit und Wartbarkeit ab?}
\end{quote}

Zur weiteren Strukturierung dieser Hauptfrage werden mehrere Unterfragen hinzugezogen:
\begin{itemize}
    \item \textbf{Qualität \& Korrektheit:} Wie qualitativ hoch ist dieser generierte Code hinsichtlich Syntax und Implementierung von Analyseaufgaben (z.\,B. Datenbereinigung, Modellierung)?
    \item \textbf{Effizienz \& Performanz:} Inwieweit entspricht der automatisch erzeugte Code modernen Standards bezüglich Laufzeit und Ressourcenverbrauch?
    \item \textbf{Wartbarkeit \& Verständlichkeit:} Wie gut lässt sich der generierte Code verstehen, dokumentieren und erweitern?
    \item \textbf{Einsatzgebiete \& Grenzen:} Für welche spezifischen Aufgaben in der Datenanalyse ist der Einsatz von LLMs sinnvoll und wo stoßen diese an ihre Grenzen?
\end{itemize}

\subsection{Relevanz der Thematik}
Die Fähigkeit, Programmiercode mit Hilfe von LLMs zu erstellen, könnte die Entwicklungsprozesse erheblich beschleunigen und neue Nutzergruppen anziehen, die bisher wenig Erfahrung mit Programmierung haben. Besonders in der Datenanalyse können viele Arbeitsschritte – vor allem wiederkehrende Aufgaben wie das Erstellen von Standard-Pipelines oder simpler Boilerplate-Code – automatisiert werden. Gleichzeitig gibt es jedoch Herausforderungen in Bezug auf Performanz, Wartbarkeit und Transparenz.

\subsection{Zielsetzung}
Das Ziel dieser Arbeit ist es, herauszufinden, wie gut moderne LLMs (Large Language Models) für die automatische Code-Generierung in der Datenanalyse mit Python geeignet sind. Dafür wird in einem Experiment Code von einem LLM generiert und mit manuell geschriebenem Code verglichen. Der Vergleich basiert auf Kriterien wie Korrektheit, Performance und Wartbarkeit. Auf Basis der Ergebnisse werden Empfehlungen für den Einsatz von LLMs in der Praxis gegeben und deren Grenzen diskutiert. Zum Schluss wird ein Ausblick darauf gegeben, wie sich diese Technologie in Zukunft weiterentwickeln könnte und welche Auswirkungen das auf Aufgaben in der Datenanalyse haben könnte\cite{chen2021evaluatinglargelanguagemodels,evalplus}.

\subsection{Aufbau der Arbeit}
Nach dieser Einleitung (Kapitel~1) folgt in Kapitel~2 eine Darstellung der \textbf{Grundlagen}. Kapitel~3 gibt einen Überblick über den aktuellen Stand der Forschung, in dem verschiedene LLM-Modelle, Publikationen und Evaluationstechniken vorgestellt werden. Darauf aufbauend wird in Kapitel~4 die \textbf{Methodik} der Arbeit erläutert. Kapitel~5 enthält dann die \textbf{Auswertung} der gewonnenen Daten sowie den Vergleich von durch ein LLM generierten und manuell erstellten Code. Kapitel~6 fasst die Ergebnisse zusammen, beantwortet die Forschungsfragen und gibt einen \textbf{Ausblick} auf weitere Entwicklungen. Schließlich enthält Kapitel~7 den \textbf{Anhang}, einschließlich Literaturverzeichnis und relevanter Dokumentationen.

% Theorieteil

\section{Grundlagen}

Im folgenden Kapitel werden die theoretischen und technischen Grundlagen vorgestellt, die für das Verständnis dieser Arbeit notwendig sind. Abschnitt~\ref{sec:LLMs} beschäftigt sich mit den Large Language Models, ihrer Funktionsweise und ihrer Bedeutung in der Code-Generierung. Danach wird in Abschnitt~\ref{sec:Python} das Potenzial der Programmiersprache Python für die Datenanalyse erläutert, bevor Abschnitt~\ref{sec:AutoCode} das Konzept der automatisierten Code-Generierung behandelt.

\subsection{Einführung in Large Language Models}
\label{sec:LLMs}
TODO: Aufbau aendern und umschreiben
\subsubsection{Grundlegendes Konzept und aktuelle Entwicklungen}
Bei großen Sprachmodellen (LLMs) handelt es sich um KI-Systeme, die mithilfe moderner Deep-Learning-Methoden entwickelt wurden, um natürliche Sprache zu verstehen und selbst Texte zu generieren \cite{naveed2024comprehensiveoverviewlargelanguage}. Dazu gehören beispielsweise Modelle wie GPT oder spezialisierte LLMs für die Code-Generierung, die auf Transformer-Architekturen basieren und sowohl für Text- als auch Code-Anwendungen optimiert sind \cite{NEURIPS2023_43e9d647, nijkamp2023codegenopenlargelanguage, 9413901}. Allerdings steigt mit der zunehmenden Größe dieser Modelle, auch der Bedarf an Rechenressourcen und großen Mengen an Trainingsdaten.
In den letzten Jahren wurden mehrere Benchmarks und Evaluierungsdatensätze speziell für die Code-Generierung entwickelt. Beispiele dafür sind \emph{HumanEval}\cite{chen2021evaluatinglargelanguagemodels} und \emph{EvalPlus}\cite{evalplus}, die genutzt werden, um die Genauigkeit und Zuverlässigkeit von LLMs in verschiedenen Programmiersprachen zu testen. Erste Studien zeigen, dass LLMs einfache bis mittelschwere Aufgaben oft vollständig lösen können. Bei komplexeren oder sehr speziellen Aufgabenbereichen stoßen sie aber noch an ihre Grenzen\cite{wang2021codet5identifierawareunifiedpretrained}.

\subsubsection{Anwendung in der Python-Programmierung}
Obwohl LLMs in vielen Sprachen Code generieren können, hat sich Python als einer der Hauptfoki herauskristallisiert. Dies liegt an der weit verbreiteten Nutzung von Python in Wissenschaft und Industrie, insbesondere in den Bereichen Datenanalyse und Machine Learning. Die umfangreichen Bibliotheken wie NumPy, pandas und scikit-learn sind Teil der Trainingskorpora, wodurch LLMs häufig in der Lage sind, Standardroutinen oder Bibliotheksfunktionen korrekt anzuwenden\cite{chen2021evaluatinglargelanguagemodels}.

\subsection{Einführung in Python für die Datenanalyse}
\label{sec:Python}

\subsubsection{Bedeutung und Bibliotheken}
Python ist dank seiner Syntax und aktiven Community eine der am weitesten verbreiteten Sprachen für Datenanalyse \cite{wang2021codet5identifierawareunifiedpretrained}. Wichtige Bibliotheken wie:
\begin{itemize}
    \item \textbf{pandas} -- Datenstrukturen und -bearbeitung,
    \item \textbf{NumPy} -- numerische Berechnungen,
    \item \textbf{scikit-learn} -- Machine-Learning-Algorithmen,
    \item \textbf{Matplotlib}, \textbf{seaborn} -- Visualisierung,
\end{itemize}
stellen ein reichhaltiges Ökosystem dar, das die effiziente Umsetzung datengetriebener Projekte ermöglicht. Viele davon werden bereits in LLM-Trainings berücksichtigt, wodurch generierter Code auf bekannte Funktionen zurückgreifen kann \cite{evalplus}.

\subsubsection{Typische Schritte einer Datenanalyse}
Eine klassische Datenanalyse in Python kann grob in sechs Schritte unterteilt werden:
\begin{enumerate}
    \item \textit{Datenimport} (z.\,B. CSV-Dateien, Datenbanken, APIs),
    \item \textit{Datenbereinigung} (fehlende Werte, Duplikate, Datentypen),
    \item \textit{Explorative Analyse und Visualisierung} (Statistiken, Plots),
    \item \textit{Feature Engineering} (Neue Variablen, Skalierung, Kodierung),
    \item \textit{Modellierung} (Trainieren und Evaluieren von ML-Modellen),
    \item \textit{Kommunikation} (Ergebnisse präsentieren, Dokumentation).
\end{enumerate}
Im Rahmen dieser Arbeit wird untersucht, ob LLMs diese Schritte automatisieren können und an welchen Stellen manuell eingegriffen werden muss \cite{chen2021evaluatinglargelanguagemodels,evalplus}.

\subsection{Automatisierte Code-Generierung für Datenanalyse}
\label{sec:AutoCode}

\subsubsection{Funktionsweise und Vorteile}
Automatisierte Code-Generierung mithilfe von LLMs basiert auf \emph{Prompts}, also Benutzeranfragen in natürlicher Sprache. Im Gegensatz zu traditionellen Code-Generatoren, die häufig starre Templates oder regellastige Systeme verwenden, können LLMs sich flexibel an den Kontext anpassen \cite{chen2021evaluatinglargelanguagemodels}. Insbesondere in datenanalytischen Szenarien, in denen standardisierte Skripte (z.\,B. für das Einlesen und Bereinigen von Daten) immer wieder benötigt werden, kann dies zu einer erheblichen Zeitersparnis führen.

\subsubsection{Herausforderungen und Grenzen}
Trotz beeindruckender Fortschritte stößt die automatisierte Code-Generierung noch häufig an Grenzen \cite{nijkamp2023codegenopenlargelanguage,wang2021codet5identifierawareunifiedpretrained}:
\begin{itemize}
    \item \textbf{Komplexe Datenstrukturen}: LLMs zeigen teils Schwächen bei Aufgaben mit hochgradiger Komplexität oder domänenspezifischem Wissen.
    \item \textbf{Performanz}: Generierter Code ist nicht immer optimal hinsichtlich Laufzeit oder Speicherverbrauch.
    \item \textbf{Wartbarkeit}: Kommentare, klare Code-Struktur und Dokumentation fehlen häufig.
    \item \textbf{Fehleranfälligkeit}: Auch Code, der zunächst lauffähig erscheint, kann subtile Bugs oder Sicherheitslücken enthalten.
\end{itemize}

Wie stark diese Faktoren in der Praxis ins Gewicht fallen, wird in den kommenden Kapiteln anhand einer empirischen Untersuchung (LLM-generierter vs. manuell erstellter Code) analysiert.
\newpage

\section{LLMs in der Programmierung – aktueller Stand}
Die Entwicklung von Large Language Models (LLMs) hat in den letzten Jahren nicht nur die Art und Weise, wie natürliche Sprache verarbeitet und generiert wird, verändert, sondern auch große Fortschritte in der automatisierten Code-Erstellung ermöglicht. Durch die Kombination aus leistungsstarken Modellarchitekturen wie Transformers, großen Mengen an Trainingsdaten und moderner Hardware sind LLMs heute fester Bestandteil vieler Bereiche der Softwareentwicklung und Datenanalyse\cite{9413901}.
In diesem Kapitel werden die aktuellen Entwicklungen und verfügbaren Modelle vorgestellt. Außerdem wird ein Überblick über ihre Einsatzmöglichkeiten in der Softwareentwicklung und Datenanalyse gegeben. Zum Schluss werden wichtige Studien und Arbeiten zur Code-Generierung betrachtet, darunter etwa die von Chen et al. (2021) vorgestellte Arbeit zu Codex, einem Modell, das speziell für die automatisierte Programmierung entwickelt wurde \cite{chen2021evaluatinglargelanguagemodels}.
TODO: umschreiben
\subsection{Überblick und Vergleich von verschiedenen LLMs}
Derzeit existiert eine Vielzahl an LLMs, darunter auch viele, die gezielt zur Code-Generierung entwickelt wurden. Zu den bekanntesten Beispielen zählen ChatGPT (GPTo1 als das modernste Modell), OpenAI Codex, Code Llama\cite{rozière2024codellamaopenfoundation}, StarCoder \cite{li2023starcodersourceyou}, CodeT5\cite{wang2021codet5identifierawareunifiedpretrained} oder CodeGen\cite{nijkamp2023codegenopenlargelanguage}. Diese Modelle teilen sich häufig folgende Merkmale:
\begin{enumerate}
    \item \textbf{Transformer-Architektur}: Nahezu alle modernen LLMs beruhen auf dem Transformer-Modell, das mithilfe von Self-Attention Mechanismen Zusammenhänge in sequentiellen Daten (Text/Code) erfassen kann\cite{9413901}.
    \item \textbf{Große Parameteranzahl}: Typische LLMs verfügen über hunderte Millionen bis mehrere Milliarden Parameter und benötigen entsprechend umfangreiche Trainingsdaten, zu denen in vielen Fällen öffentlich verfügbare Code-Repositories (z. B. GitHub) zählen. TODO: umschreiben
    \item \textbf{Breite Sprachenunterstützung}: Neben Python werden häufig C++, Java, JavaScript und andere Programmiersprachen abgedeckt. Python nimmt jedoch oft eine zentrale Rolle ein, da sie im Bereich Datenanalyse und Machine Learning weit verbreitet ist. TODO: Umschreiben und Quelle hinzufuegen
\end{enumerate}
Ein Vergleich der LLMs lässt sich anhand verschiedener Kriterien vornehmen:
\begin{itemize}
    \item \textbf{Größe und Trainingsdaten}: Modelle wie GPT-4 oder Code Llama sind mit einer Vielzahl an Code-Datensätzen trainiert und erreichen dadurch in Benchmarks eine hohe Treffsicherheit. TODO: Quelle hinzufuegen
    \item \textbf{Lizenz und Offenheit}: Während GitHub Copilot und ChatGPT proprietär sind, existieren mit Code Llama, StarCoder oder CodeGen auch teils offene bzw. frei nutzbare Alternativen.
    \item \textbf{Spezialisierung}: Einige Modelle sind speziell auf Code-Generierung abgestimmt (z. B. Code Llama-Python, StarCoder), während andere (z. B. ChatGPT) einen generellen Sprachkontext haben, der sich jedoch auch auf Code-Aufgaben anwenden lässt.
\end{itemize}
\subsection{Einsatzgebiete von LLMs in der Programmierung}
\label{sec:einsatzgebiete}

Die zunehmende Leistungsfähigkeit von Large Language Models (LLMs) ermöglicht es, Programmieraufgaben in diversen Bereichen zu automatisieren oder zu beschleunigen. Häufig genannte \emph{Einsatzgebiete} sind dabei:
TODO: Quellen checken
\begin{itemize}
  \item \textbf{Autovervollständigung und Boilerplate-Code}:  
  Integriert in Entwicklungsumgebungen wie Visual Studio Code oder JetBrains können Tools wie GitHub Copilot Standardroutinen vorschlagen und repetitive Abläufe deutlich verkürzen \cite{Li2022AlphaCode}.

  \item \textbf{Refactoring und Fehlersuche}:  
  Dank ihrer Kontextsensitivität können LLMs bestehenden Quellcode analysieren und an einigen Stellen optimierte oder korrigierte Varianten vorschlagen \cite{Zhang2023CodexRevisited}. Dadurch lassen sich potenzielle Bugs oder ineffiziente Strukturen frühzeitig erkennen.

  \item \textbf{Automatisierte Dokumentation und Code-Kommentierung}:  
  Modelle wie ChatGPT oder Code Llama-Python bieten die Möglichkeit, vorhandenen Code zu erklären oder zu kommentieren, was die Wartung und Teamkommunikation verbessert \cite{Phung2023MultiTask}.

  \item \textbf{Datenanalyse und Machine Learning}:  
  Im Fokus vieler Python-Anwender steht die Datenverarbeitung mit Bibliotheken wie \texttt{pandas}, \texttt{NumPy} oder \texttt{scikit-learn}. LLMs können hier einfache Skripte für das Einlesen, Transformieren und Visualisieren von Daten generieren \cite{Wang2023DataCentric}, wodurch gerade Einsteiger schnell produktiv werden.

  \item \textbf{Rapid Prototyping}:  
  In frühen Entwicklungsphasen nutzen Entwickler LLMs als Assistent, um zügig Prototypen zu erstellen. Die Modelle liefern Vorschläge für Projektstrukturen, Abhängigkeiten und Testumgebungen.
\end{itemize}

Obwohl diese Einsatzgebiete großes Potenzial bieten, sind LLMs nicht fehlerfrei. Gerade bei komplexen architektonischen Entscheidungen oder domänenspezifischen Anforderungen ist das \emph{menschliche Fachwissen} weiterhin essenziell, um die Qualität und Wartbarkeit des Codes zu gewährleisten.

\subsection{Vergangene Studien und Arbeiten zur Code-Generierung}
\label{sec:vergangene_studien}

Die Forschung zur automatisierten Code-Generierung hat in den letzten Jahren eine rasante Entwicklung erlebt, wobei Arbeiten aus den Bereichen \emph{Software Engineering}, \emph{Natural Language Processing} und \emph{Machine Learning} zusammenfließen. Im Folgenden werden einige zentrale Punkte hervorgehoben:

\begin{enumerate}
  \item \textbf{Frühe Ansätze und Expertensysteme}  
  Bereits in den 1980er-Jahren experimentierten Wissenschaftler mit regelbasierten Generatoren, die für bestimmte Domänen (z.\,B. Datenbankzugriffe) begrenzte Codefragmente erzeugten. Diese Systeme waren allerdings sehr spezialisiert und kaum flexibel \cite{Henderson2023LLMRefactor}.

  \item \textbf{Tiefe neuronale Modelle und Sequenz-zu-Sequenz-Architekturen}  
  Mit dem Aufkommen tief lernender Methoden (LSTMs, GRUs) in der NLP-Forschung rückte die Idee in den Fokus, \emph{natürlichsprachliche Beschreibungen} in Programmcode zu übersetzen. Spätere Transformer-basierte Modelle wie GPT, Codex oder Code Llama zeigten, dass eine größere Datenbasis (z.\,B. GitHub-Repositories) die Qualität der Code-Generierung erheblich steigern kann \cite{AlphaCodeDeepMind}.

  \item \textbf{Benchmarks und Evaluierungen}  
  Um generierten Code objektiv zu messen, wurden standardisierte Benchmarks wie \emph{HumanEval} \cite{OpenAI2021} oder \emph{EvalPlus} \cite{EvalPlus2023} entwickelt. Sie testen, inwieweit LLMs funktional korrekten und performant lauffähigen Code liefern. Aktuelle Studien erweitern diese Benchmarks auf komplexere Szenarien, z.\,B. Wettbewerbsniveau („AlphaCode“) \cite{Li2022AlphaCode} oder domänenspezifische Datenanalyse \cite{Wang2023DataCentric}.

  \item \textbf{Aktuelle Forschungsschwerpunkte}  
  Zeitgenössische Arbeiten untersuchen u.\,a. die \emph{Fehlerquote} (z.\,B. Syntaxfehler vs. semantische Fehler), die \emph{Wartbarkeit} (z.\,B. Code-Kommentierung) sowie die \emph{Performanz} (z.\,B. Laufzeit, Speicherverbrauch) des generierten Codes. Insbesondere in größeren Projekten bleibt menschliches Eingreifen unverzichtbar, da LLMs gerade bei sehr umfangreichen oder domänenspezifischen Anwendungen an ihre Grenzen stoßen \cite{Zhang2023CodexRevisited}.

\end{enumerate}

Zusammenfassend zeigen die bisherigen Studien, dass LLMs zwar weitreichendes Potenzial zur \emph{automatisierten Code-Generierung} besitzen, jedoch kein vollständiger Ersatz für erfahrene Entwickler sind. Insbesondere bei sicherheitskritischen Anwendungen oder stark branchenspezifischen Projekten empfiehlt sich ein sorgfältiges Code-Review durch Experten. Nichtsdestotrotz deutet die Entwicklung darauf hin, dass LLMs das Potenzial haben, den Programmieralltag nachhaltig zu verändern – vom automatisierten Erzeugen kleiner Module bis hin zur Unterstützung bei komplexen Datenanalyseprozessen.

\section{Methodik}
\label{sec:methodik}
\subsection{Vorgehensweise der Untersuchung}
    In der Untersuchung soll geprüft werden, inwieweit Large Language Models in der Lage sind gängige Datenanalyse-Schritte auf Grundlage eines gegebenen Datensatzes durchzuführen. Hierbeui wird ChatGPT als aktueller Marktführer mit dem Sprachmodell GPTo1 verwendet, welches das neueste Modell ist. Ebenso gilt es herauszufinden wie qualitativ und effizient diese Lösung ist. Hierbei bezieht es sich auf die Forschungsfragen aus Kapitel 1~\ref{sec:forschungsfragen}.
    Die Vorgehensweise hierbei ist wie folgendermaßen: Zuerst wird der verwendete Datensatz von Berlin Open Data an das Modell übergeben und dazu eine Prompt. Diese Prompts können in Kapitel 4.2~\ref{sec:testfaelle} eingesehen werden. Anschließend wird der generierte Code mit HumanEval evaluiert um die generelle Qualität des Codes zu bewerten. Daraufhin wird der Code ausgeführt und die benötigte Laufzeit gemessen. Abschließend werden die Ergebnisse des Ausführung bewertet. (TODO: Tool zur Auswertung der Ergebnisse oder Vergleich mit manuellem Skript)
    Die Ergebnisse der Auswertung mit \emph{EvalPlus}\cite{evalplus} werden in Kapitel 5~\ref{sec:auswertung} detailliert dargestellt und auch mit den Ergebnissen anderer Arbeiten, wie etwa von Chen et al. (2021)\cite{chen2021evaluatinglargelanguagemodels} und Liu et al. (2023)\cite{NEURIPS2023_43e9d647} verglichen.

\subsection{Testfälle der Datenanalyse}
\label{sec:testfaelle}
\subsubsection{Testfall 1}
    Im ersten Testfall soll der Datensatz nach einer gewissen Spalte sortiert werden. Die Begründung hierfür ist, dass dies eine sehr einfache, aber auch sehr häufig aufkommende Datenanalyse-Aufgabe ist und somit einen guten Einstieg in die Untersuchung darstellt. Die Prompt für diese Aufgabe lautet: \emph{Erstelle mir ein Python Skript, mit welchem der Datensatz nach der Anzahl der Straftaten insgesamt eines Bezirks sortiert wird.}.

\subsubsection{Testfall 2}
    Aufbauend auf Testfall eins, soll im zweiten Testfall eine simple Visualisierung des Datensatzes stattfinden. Es soll ein Balkendiagramm der Bezirke und wieder der Straftaten insgesamt erstellt werden. Hierbei sollen die Bezirke auf der x-Achse und die Anzahl der Straftaten auf der y-Achse dargestellt und absteigend mit der Anzahl der Bezirke sortiert werden. Die Prompt für diese Aufgabe lautet: \emph{Erstelle mir ein Python Skript, mit welchem ein Balkendiagramm der Anzahl der Straftaten insgesamt pro Bezirk erstellt wird. Hierbei sollen auf der X-Achse die Bezirke und auf der Y-Achse die Anzahl der Straftaten sein.}.

\subsubsection{Testfall 3}
    Im dritten Testfall soll das Sprachmodell die prozentuale Verteilung der Straftaten in den Bezirken berechnen. Die Prompt für diese Aufgabe lautet: \emph{Erstelle mir ein Python Skript, mit welchem die prozentuale Verteilung der genauen Straftaten anteilig der Straftaten insgesamt pro Bezirk berechnet wird.}.


\subsection{Auswertungskriterien}
\label{sec:auswertungskriterien}
    Die Auswertung der generiertes Python Skripte erfolgt anhand der in Kapitel 1~\ref{sec:forschungsfragen} definierten Kriterien. Hierfür wird zum einen das Tool \emph{EvalPlus}\cite{evalplus} verwendet, welches eine standardisierte Bewertung zur Korrektheit des generierten Codes ermöglicht. Ebenso werden Laufzeit und Ressourcennutzung des Codes bewertet, um die Effizienz des Skripts zu bemessen.
    Anschließend wird der Code manuell auf Wartbarkeit und Verständlichkeit geprüft. Hierbei wird insbesondere auf die Strukturierung des Codes, die Kommentierung und die Dokumentation geachtet. Die Ergebnisse der Auswertung werden in Kapitel 5~\ref{sec:auswertung} detailliert dargestellt und bewertet.

\subsection{Verwendete Tools}
    \begin{enumerate}
        \item \textbf{Large Language Model}: Als Large Language Model wird ChatGPT mit GPTo1 verwendet, da ChatGPT als aktueller Marktführer gilt und GPTo1 das neueste Modell ist.
        \item \textbf{Evaluierungsframework}: Zur Bewertung der generierten Python Skripte wird das Evaluierungsframework \emph{EvalPlus}\cite{evalplus} verwendet, welches eine standardisierte Bewertung zur Korrektheit des generierten Codes ermöglicht.
    \end{enumerate}



\newpage
% Literatur

\printbibliography

\end{document}

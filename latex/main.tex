\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}    % Umlaute in der .tex-Datei
\usepackage[T1]{fontenc}       % Korrekte Silbentrennung
\usepackage{lmodern}           % Bessere Schrift
\usepackage{graphicx}          % Für Logo/Grafiken
\usepackage[left=3.5cm, right=2.0cm, top=2.5cm, bottom=2.5cm]{geometry} % Seitenränder anpassen bei Bedarf
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{setspace}
\onehalfspacing
\definecolor{htwgreen}{HTML}{76B900}
\usepackage[ngerman, num]{isodate}
\monthyearsepgerman{\,}{\,}
\usepackage[ngerman]{babel}
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{./references.bib} 

% Für Überschriften etwas schönere Darstellung (optional):
% \usepackage{titlesec}
% \titleformat{\section}{\bfseries\Large}{\thesection}{1em}{}
% \titleformat{\subsection}{\bfseries\normalsize}{\thesubsection}{1em}{}

% \onehalfspacing % 1,5-zeiliger Abstand (ggf. anpassen)

\begin{document}

%--------------------------
% Titelseite
%--------------------------
\begin{titlepage}
    \centering
    
    \includegraphics[width=4cm]{./bilder/S04_HTW_Berlin_Logo_pos_FARBIG_RGB.jpg}\\[1.0cm]
    \rule{\linewidth}{0.5pt}\\[0.7cm]
    
    {\color{htwgreen}\bfseries\Large Eignung von Large Language Models (LLMs) zur Generierung von Python Code zur
    Datenanalyse}\\[0.5cm]
    \rule{\linewidth}{0.5pt}\\[2.0cm]
    {\large Bachelorarbeit/Masterarbeit}\\[1.5cm]
    
    % Name des Studiengangs, Fachbereich, etc.
    {\large Name des Studiengangs}\\
    {\LARGE Wirtschaftsinformatik}\\[0.3cm]
    {\color{htwgreen}\LARGE \textbf{Fachbereich 4}}\\[1.5cm]
    
    {vorgelegt von}\\
    {\LARGE Maurice Krüger}\\[3cm]
    
    % Datum, Ort
    {\Large Datum:}\\
    Berlin, \today\\[2.5cm]

    % Gutachter
    {\LARGE
    \begin{tabular}{l l}
        Erstgutachter:  & Prof. Dr. Ingo Claßen \\
        Zweitgutachter: & Prof. Dr. Axel Hochstein \\
    \end{tabular}
    }

\end{titlepage}

%--------------------------
% Inhaltsverzeichnis
%--------------------------
\tableofcontents
\newpage

%--------------------------
% Einleitung
%--------------------------
\section{Einleitung}
\subsection{Problemstellung und Forschungsfragen}

Die rasante Entwicklung von Large Language Models (LLMs), wie beispielsweise ChatGPT, hat in den vergangenen Jahren sowohl im privaten als auch im beruflichen Umfeld für große Aufmerksamkeit gesorgt. Während LLMs ursprünglich vor allem zur Verarbeitung und Generierung natürlicher Sprache eingesetzt wurden, zeigt sich zunehmend, dass sie auch Programmiercode in diversen Sprachen erzeugen können. Insbesondere für Python -- eine häufig genutzte Sprache für Datenanalyse und Machine Learning -- sind die Fortschritte in der automatisierten Code-Generierung bereits beachtlich \cite{web:1,web:2}.

Aktuelle Forschungsarbeiten befassen sich mit der systematischen Evaluation solcher Code-Generierungen, um Fehlerquellen und Qualitätsmerkmale quantifizieren zu können \cite{web:3,web:4}. Die Bereitstellung öffentlicher Evaluierungsdatensätze und -frameworks, wie zum Beispiel \emph{HumanEval} oder \emph{EvalPlus}, ermöglicht standardisierte Vergleichsstudien verschiedener LLMs. Dies eröffnet neue Anwendungsfelder im Bereich der Datenanalyse: Anstatt den Code manuell zu schreiben, könnten Anwender in Zukunft lediglich ihre Anforderungen in natürlicher Sprache formulieren und vom Modell automatisch umsetzen lassen \cite{web:5}.

Vor diesem Hintergrund stellt sich die Frage, \emph{ob und inwiefern} LLMs tatsächlich qualitativ hochwertigen Python-Code für datenanalytische Aufgaben erzeugen können und wie dieser Code im Vergleich zu manuell erstellten Skripten abschneidet. Auch mögliche Grenzen dieser automatisierten Generierung, etwa in Bezug auf Performanz, Wartbarkeit oder Fehlerraten, spielen hierbei eine zentrale Rolle \cite{web:6}.

Daraus ergibt sich die zentrale \textbf{Hauptforschungsfrage}:

\begin{quote}
    \emph{Inwieweit eignet sich die automatisierte Code-Generierung durch Large Language Models (LLMs) 
    zur Durchführung gängiger Datenanalyseaufgaben in Python, und wie schneidet dieser Code 
    im Vergleich zu manuell geschriebenen Skripten hinsichtlich Effizienz, Korrektheit und Wartbarkeit ab?}
\end{quote}

Zur weiteren Strukturierung dieser Hauptfrage werden mehrere Unterfragen hinzugezogen:
\begin{itemize}
    \item \textbf{Qualität \& Korrektheit:} Wie hoch ist die Korrektheit des generierten Codes hinsichtlich Syntax und Implementierung von Analyseaufgaben (z.\,B. Datenbereinigung, Modellierung)?
    \item \textbf{Effizienz \& Performanz:} Inwieweit entspricht der automatisch erzeugte Code modernen Standards bezüglich Laufzeit und Ressourcenverbrauch?
    \item \textbf{Wartbarkeit \& Verständlichkeit:} Wie gut lässt sich der generierte Code verstehen, dokumentieren und erweitern?
    \item \textbf{Einsatzgebiete \& Grenzen:} Für welche spezifischen Aufgaben in der Datenanalyse ist der Einsatz von LLMs sinnvoll, und wo stößt die Technologie an ihre Grenzen?
\end{itemize}

\subsection{Relevanz der Thematik}
Die Möglichkeit, Programmiercode mithilfe von LLMs zu generieren, könnte Entwicklungsprozesse signifikant beschleunigen und neue Nutzergruppen ansprechen, die bislang nur wenig Erfahrung mit Programmierung haben. Gerade in der Datenanalyse können viele Arbeitsschritte -- insbesondere repetitive Abläufe wie das Schreiben von Standard-Pipelines oder Boilerplate-Code -- automatisiert werden. Gleichzeitig ergeben sich jedoch Herausforderungen bezüglich \emph{Performanz}, \emph{Wartbarkeit} und \emph{Transparenz} \cite{web:2}.

\subsection{Zielsetzung}
Ziel dieser Arbeit ist eine \textbf{systematische Untersuchung}, wie gut sich moderne LLMs für die automatisierte Code-Generierung im Bereich der Python-Datenanalyse eignen. Dazu wird in einem empirischen Experiment Code durch ein LLM erzeugt und mit manuell geschriebenem Code verglichen. Dieser Vergleich erfolgt anhand definierter Kriterien wie \emph{Korrektheit}, \emph{Performance} und \emph{Wartbarkeit}. Auf Basis der Ergebnisse werden Handlungsempfehlungen für den praktischen Einsatz abgeleitet und Grenzen der Technologie aufgezeigt. Ein abschließender \emph{Zukunftsausblick} beleuchtet mögliche Weiterentwicklungen im Bereich der LLMs und deren Einfluss auf datenanalytische Aufgaben \cite{web:3,web:4}.

\subsection{Aufbau der Arbeit}
Nach dieser Einleitung (Kapitel~1) folgt in Kapitel~2 eine Darstellung der \textbf{Grundlagen}. Kapitel~3 gibt einen Überblick über den aktuellen Stand der Forschung, in dem verschiedene LLM-Modelle, Publikationen und Evaluationstechniken vorgestellt werden. Darauf aufbauend wird in Kapitel~4 die \textbf{Methodik} der Arbeit erläutert. Kapitel~5 enthält dann die \textbf{Auswertung} der gewonnenen Daten sowie den Vergleich von LLM-generiertem und manuell erstelltem Code. Kapitel~6 fasst die Ergebnisse zusammen, beantwortet die Forschungsfragen und gibt einen \textbf{Ausblick} auf weitere Entwicklungen. Schließlich enthält Kapitel~7 den \textbf{Anhang}, einschließlich Literaturverzeichnis und relevanter Dokumentationen.

%--------------------------
% Theorieteil
%--------------------------
\section{Grundlagen}

Im folgenden Kapitel werden die theoretischen und technischen Grundlagen dargelegt, die zum Verständnis dieser Arbeit erforderlich sind. Abschnitt~\ref{sec:LLMs} widmet sich den Large Language Models, deren Funktionsweise und ihrer Rolle in der Code-Generierung. Anschließend wird in Abschnitt~\ref{sec:Python} das Potenzial der Programmiersprache Python für Datenanalyse erläutert, bevor Abschnitt~\ref{sec:AutoCode} sich dem Konzept der automatisierten Code-Generierung zuwendet.

\subsection{Einführung in Large Language Models}
\label{sec:LLMs}

\subsubsection{Grundlegendes Konzept und aktuelle Entwicklungen}
Large Language Models (LLMs) sind KI-Modelle, die mithilfe moderner Deep-Learning-Architekturen darauf trainiert werden, Sprache zu verstehen und zu generieren \cite{web:2}(TODO: quelle 1 auch?). Moderne Modelle wie GPT oder Code-spezifische LLMs beruhen oft auf Transformer-Architekturen, die sich sowohl zur Text- als auch zur Code-Generierung eignen \cite{web:5}. Mit zunehmender Größe der Modelle, die teils mehrere hundert Milliarden Parameter umfassen, steigt jedoch auch der Bedarf an Rechenleistung und Trainingsdaten.

Verschiedene Forschungsarbeiten haben in den letzten Jahren spezielle \emph{Benchmarks} und \emph{Evaluierungsdatensätze} für Code-Generierung entwickelt. Beispiele sind \emph{HumanEval} \cite{web:3} und \emph{EvalPlus} \cite{web:4}, anhand derer die Genauigkeit und Robustheit der LLMs in unterschiedlichen Programmiersprachen gemessen werden kann. Erste Studien zeigen, dass LLMs bereits fähig sind, einfache bis mittelschwere Aufgaben vollständig zu lösen, während bei komplexeren und domänenspezifischen Szenarien noch deutliche Leistungsdefizite zu beobachten sind \cite{web:6}.

\subsubsection{Anwendung in der Python-Programmierung}
Obgleich LLMs in zahlreichen Sprachen Code generieren können, hat sich Python als einer der Schwerpunkte herauskristallisiert. Dies liegt an der Verbreitung von Python in Wissenschaft und Industrie, insbesondere für Datenanalyse und Machine Learning \cite{web:5}. Die umfangreichen Bibliotheken (z.\,B. NumPy, pandas, scikit-learn) fließen in die Trainingskorpora ein, sodass LLMs häufig bereits Standardroutinen oder Bibliotheksfunktionen korrekt anwenden \cite{web:2}.

\subsection{Einführung in Python für die Datenanalyse}
\label{sec:Python}

\subsubsection{Bedeutung und Bibliotheken}
Python ist dank seiner Syntax und aktiven Community eine der am weitesten verbreiteten Sprachen für Datenanalyse \cite{web:6}. Wichtige Bibliotheken wie:
\begin{itemize}
    \item \textbf{pandas} -- Datenstrukturen und -bearbeitung,
    \item \textbf{NumPy} -- numerische Berechnungen,
    \item \textbf{scikit-learn} -- Machine-Learning-Algorithmen,
    \item \textbf{Matplotlib}, \textbf{seaborn} -- Visualisierung,
\end{itemize}
stellen ein reichhaltiges Ökosystem dar, das die effiziente Umsetzung datengetriebener Projekte ermöglicht. Viele davon werden bereits in LLM-Trainings berücksichtigt, wodurch generierter Code auf bekannte Funktionen zurückgreifen kann \cite{web:4}.

\subsubsection{Typische Schritte einer Datenanalyse}
Eine klassische Datenanalyse in Python kann grob in sechs Schritte unterteilt werden:
\begin{enumerate}
    \item \textit{Datenimport} (z.\,B. CSV-Dateien, Datenbanken, APIs),
    \item \textit{Datenbereinigung} (fehlende Werte, Duplikate, Datentypen),
    \item \textit{Explorative Analyse und Visualisierung} (Statistiken, Plots),
    \item \textit{Feature Engineering} (Neue Variablen, Skalierung, Kodierung),
    \item \textit{Modellierung} (Trainieren und Evaluieren von ML-Modellen),
    \item \textit{Kommunikation} (Ergebnisse präsentieren, Dokumentation).
\end{enumerate}
Im Rahmen dieser Arbeit wird untersucht, ob LLMs diese Schritte automatisieren können und an welchen Stellen manuell eingegriffen werden muss \cite{web:3,web:4}.

\subsection{Automatisierte Code-Generierung für Datenanalyse}
\label{sec:AutoCode}

\subsubsection{Funktionsweise und Vorteile}
Automatisierte Code-Generierung mithilfe von LLMs basiert auf \emph{Prompts}, also Benutzeranfragen in natürlicher Sprache. Im Gegensatz zu traditionellen Code-Generatoren, die häufig starre Templates oder regellastige Systeme verwenden, können LLMs sich flexibel an den Kontext anpassen \cite{web:2}. Insbesondere in datenanalytischen Szenarien, in denen standardisierte Skripte (z.\,B. für das Einlesen und Bereinigen von Daten) immer wieder benötigt werden, kann dies zu einer erheblichen Zeitersparnis führen.

\subsubsection{Herausforderungen und Grenzen}
Trotz beeindruckender Fortschritte stößt die automatisierte Code-Generierung noch häufig an Grenzen \cite{web:5,web:6}:
\begin{itemize}
    \item \textbf{Komplexe Datenstrukturen}: LLMs zeigen teils Schwächen bei Aufgaben mit hochgradiger Komplexität oder domänenspezifischem Wissen.
    \item \textbf{Performanz}: Generierter Code ist nicht immer optimal hinsichtlich Laufzeit oder Speicherverbrauch.
    \item \textbf{Wartbarkeit}: Kommentare, klare Code-Struktur und Dokumentation fehlen häufig.
    \item \textbf{Fehleranfälligkeit}: Auch Code, der zunächst lauffähig erscheint, kann subtile Bugs oder Sicherheitslücken enthalten.
\end{itemize}

Wie stark diese Faktoren in der Praxis ins Gewicht fallen, wird in den kommenden Kapiteln anhand einer empirischen Untersuchung (LLM-generierter vs. manuell erstellter Code) analysiert.

%--------------------------
% Literatur
%--------------------------
\printbibliography

\end{document}
